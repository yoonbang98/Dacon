{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNpV5Artn60Pp+iwMokH9Pa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QHfw6Jz8I_Ey","executionInfo":{"status":"ok","timestamp":1663067530643,"user_tz":-540,"elapsed":28486,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"1f9269aa-9b22-45b1-a154-d46b6ff347aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# %cd /content/drive/MyDrive/Colab Notebooks/Dacon/samsung_ai/data/\n","\n","# !unzip -qq \"/content/drive/MyDrive/Colab Notebooks/Dacon/samsung_ai/data/data.zip\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"djly_4_LyW2R","executionInfo":{"status":"ok","timestamp":1662082049223,"user_tz":-540,"elapsed":301820,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"88148242-2460-4ecc-ab92-be562eeae4d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Dacon/samsung_ai/data\n"]}]},{"cell_type":"code","source":["! pip install rdkit\n","!pip install transformers datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iillEmrJJLXO","executionInfo":{"status":"ok","timestamp":1663067554405,"user_tz":-540,"elapsed":23771,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"4aa82243-4eed-45dd-aa18-776ab60067ca"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting rdkit\n","  Downloading rdkit-2022.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.8 MB)\n","\u001b[K     |████████████████████████████████| 36.8 MB 50.4 MB/s \n","\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rdkit) (1.21.6)\n","Installing collected packages: rdkit\n","Successfully installed rdkit-2022.3.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 4.0 MB/s \n","\u001b[?25hCollecting datasets\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[K     |████████████████████████████████| 365 kB 71.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 39.1 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 49.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 64.5 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 66.5 MB/s \n","\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 49.9 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, tokenizers, responses, multiprocess, huggingface-hub, transformers, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.4.0 huggingface-hub-0.9.1 multiprocess-0.70.13 responses-0.18.0 tokenizers-0.12.1 transformers-4.21.3 urllib3-1.25.11 xxhash-3.0.0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import random\n","import os\n","\n","from rdkit import Chem \n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from transformers import AutoTokenizer, AdamWeightDecay, AutoModelForSequenceClassification,AutoModel, AutoConfig, AutoTokenizer, TrainingArguments, Trainer"],"metadata":{"id":"VvqDFdJ2u1Ov","executionInfo":{"status":"ok","timestamp":1663067560763,"user_tz":-540,"elapsed":6369,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def seed_everything(seed: int = 42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)  # type: ignore\n","    torch.backends.cudnn.deterministic = True  # type: ignore\n","    torch.backends.cudnn.benchmark = True  # type: ignore\n","\n","seed_everything()"],"metadata":{"id":"1ga2tzAtvKUc","executionInfo":{"status":"ok","timestamp":1663067560764,"user_tz":-540,"elapsed":13,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import os\n","path2 = '/content/drive/MyDrive/Colab Notebooks/Dacon/samsung_ai/data/data/mol_files/'\n","\n","print(os.path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-uRPSVP2XryE","executionInfo":{"status":"ok","timestamp":1663067560765,"user_tz":-540,"elapsed":13,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"7230f995-b242-4e4d-8ef1-a7ad123a24a8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["<module 'posixpath' from '/usr/lib/python3.7/posixpath.py'>\n"]}]},{"cell_type":"code","source":["# from glob import glob\n","# import re\n","# train_mol = sorted(glob(path +\"train_set/*.mol\"))"],"metadata":{"id":"qveOd03fb4vl","executionInfo":{"status":"ok","timestamp":1662219848775,"user_tz":-540,"elapsed":32,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from rdkit.Chem import AllChem\n","from rdkit import Chem, DataStructs\n","class ECFP6:\n","    def __init__(self, smiles):\n","        self.mols = [Chem.MolFromSmiles(i) for i in smiles]\n","        self.smiles = smiles\n","\n","    def mol2fp(self, mol, radius = 5):\n","        fp = AllChem.GetMorganFingerprintAsBitVect(mol, nBits = 2048,radius = radius)\n","        array = np.zeros((1,))\n","        DataStructs.ConvertToNumpyArray(fp, array)\n","        return array\n","\n","    def compute_ECFP6(self, name):\n","        bit_headers = ['bit' + str(i) for i in range(2048)]\n","        arr = np.empty((0,2048), int).astype(int)\n","        for i in self.mols:\n","            fp = self.mol2fp(i)\n","            arr = np.vstack((arr, fp))\n","        df_ecfp6 = pd.DataFrame(np.asarray(arr).astype(int),columns=bit_headers)\n","        df_ecfp6.insert(loc=0, column='smiles', value=self.smiles)\n","        df_ecfp6.to_csv(path2 + name +'_ECFP6.csv', index=False)"],"metadata":{"id":"77nghuPVbzqw","executionInfo":{"status":"ok","timestamp":1663069291314,"user_tz":-540,"elapsed":536,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/Colab Notebooks/Dacon/samsung_ai/data/'"],"metadata":{"id":"Iq9WBcopJV32","executionInfo":{"status":"ok","timestamp":1663067560766,"user_tz":-540,"elapsed":11,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv(path + 'train_set.ReorgE.csv', index_col = 0)\n","test = pd.read_csv(path + 'test_set.csv', index_col = 0)\n","submission = pd.read_csv(path + 'sample_submission.csv', index_col = 0)"],"metadata":{"id":"KtNtsI69Jk0t","executionInfo":{"status":"ok","timestamp":1663067565014,"user_tz":-540,"elapsed":4258,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["ecfp6_descriptor = ECFP6(train.SMILES)\n","ecfp6_descriptor.compute_ECFP6('train2048_radius5')"],"metadata":{"id":"ZA0h_r6ThUEn","executionInfo":{"status":"ok","timestamp":1663069774959,"user_tz":-540,"elapsed":467521,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["ecfp6_descriptor2 = ECFP6(test.SMILES)\n","ecfp6_descriptor2.compute_ECFP6('test2048_radius5')"],"metadata":{"id":"rx5sbH-kiT4r","executionInfo":{"status":"ok","timestamp":1663069930741,"user_tz":-540,"elapsed":1589,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["train_ecfp = pd.read_csv(path2 + 'train_ECFP6.csv')\n","test_ecfp = pd.read_csv(path2 + 'test_ECFP6.csv')"],"metadata":{"id":"BY-w8kvsy0MP","executionInfo":{"status":"ok","timestamp":1663070513891,"user_tz":-540,"elapsed":4225,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["train_x = train_ecfp.iloc[:,1:]\n","train_y = train[['Reorg_g','Reorg_ex']]\n","#train_y_ex = train[['Reorg_g','Reorg_ex']]\n","test_x = test_ecfp.iloc[:,1:]"],"metadata":{"id":"wJ5oJLdtzACh","executionInfo":{"status":"ok","timestamp":1663070515912,"user_tz":-540,"elapsed":3,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n","import lightgbm as lgb\n","LR_g_1 = lgb.LGBMRegressor(random_state=42, n_estimators=1000, max_depth=61, learning_rate=0.075)\n","LR_g_2 = lgb.LGBMRegressor(random_state=22, n_estimators=1000, max_depth=61, learning_rate=0.075)\n","LR_g_3 = lgb.LGBMRegressor(random_state=2, n_estimators=1000, max_depth=61, learning_rate=0.075)\n","\n","\n","estimators = [     ('lr1', LR_g_1),\n","    ('lr2', LR_g_2),\n","     ('lr3', LR_g_3)\n","\n","]\n","reg = StackingRegressor(\n","    estimators=estimators,\n","     #final_estimator=RandomForestRegressor(n_estimators=10,\n","                                           #random_state=42)\n",")\n","reg.fit(train_x, train_y_g)"],"metadata":{"id":"Mh-q5tAwzsJG","executionInfo":{"status":"ok","timestamp":1663069159225,"user_tz":-540,"elapsed":299206,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ef59fa19-2c05-4f95-c044-2609985b2318"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["StackingRegressor(estimators=[('lr1',\n","                               LGBMRegressor(learning_rate=0.075, max_depth=61,\n","                                             n_estimators=1000,\n","                                             random_state=42)),\n","                              ('lr2',\n","                               LGBMRegressor(learning_rate=0.075, max_depth=61,\n","                                             n_estimators=1000,\n","                                             random_state=22)),\n","                              ('lr3',\n","                               LGBMRegressor(learning_rate=0.075, max_depth=61,\n","                                             n_estimators=1000,\n","                                             random_state=2))])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["test_g = reg.predict(test_x)"],"metadata":{"id":"3LFrA6CNkxz4","executionInfo":{"status":"ok","timestamp":1663069170588,"user_tz":-540,"elapsed":529,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["test_g.max()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhmV894omBt9","executionInfo":{"status":"ok","timestamp":1663069171856,"user_tz":-540,"elapsed":5,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"10ee2f65-9ae9-4ea3-8d26-9d2f7f235150"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8802740546565183"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n","import lightgbm as lgb\n","LR_ex_1 = lgb.LGBMRegressor(random_state=42, n_estimators=1000, max_depth=61, learning_rate=0.075)\n","LR_ex_2 = lgb.LGBMRegressor(random_state=22, n_estimators=1000, max_depth=61, learning_rate=0.075)\n","LR_ex_3 = lgb.LGBMRegressor(random_state=2, n_estimators=1000, max_depth=61, learning_rate=0.075)\n","\n","\n","estimators2 = [     ('lr1', LR_ex_1),\n","    ('lr2', LR_ex_2),\n","     ('lr3', LR_ex_3)\n","\n","]\n","reg2 = StackingRegressor(\n","    estimators=estimators2,\n","    cv = 5,\n","     #final_estimator=RandomForestRegressor(n_estimators=10,\n","                                           #random_state=42)\n",")\n","reg2.fit(train_x, train_y_ex)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zs89Cyvdk99W","executionInfo":{"status":"ok","timestamp":1663068603074,"user_tz":-540,"elapsed":302405,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"1191ee1e-189b-44a3-9583-d92f4acc8043"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["StackingRegressor(cv=5,\n","                  estimators=[('lr1',\n","                               LGBMRegressor(learning_rate=0.075, max_depth=61,\n","                                             n_estimators=1000,\n","                                             random_state=42)),\n","                              ('lr2',\n","                               LGBMRegressor(learning_rate=0.075, max_depth=61,\n","                                             n_estimators=1000,\n","                                             random_state=22)),\n","                              ('lr3',\n","                               LGBMRegressor(learning_rate=0.075, max_depth=61,\n","                                             n_estimators=1000,\n","                                             random_state=2))])"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["test_ex = reg2.predict(test_x)"],"metadata":{"id":"79avZNAtk99Y","executionInfo":{"status":"ok","timestamp":1663068603076,"user_tz":-540,"elapsed":37,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["test_ex.max()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"chBk_8qOmHA2","executionInfo":{"status":"ok","timestamp":1663068604757,"user_tz":-540,"elapsed":17,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"836b7c77-8e43-49fe-d377-39ad97fe3f80"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7642979368912256"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["submission.Reorg_g = test_g\n","submission.Reorg_ex = test_ex"],"metadata":{"id":"rjm2HMQwlLu5","executionInfo":{"status":"ok","timestamp":1663068604758,"user_tz":-540,"elapsed":14,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["submission"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"pshkX7Dclkm5","executionInfo":{"status":"ok","timestamp":1663068604759,"user_tz":-540,"elapsed":14,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"73a8f645-772e-4b9f-82f4-5f7cf6c5e942"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           Reorg_g  Reorg_ex\n","index                       \n","test_0    0.350062  0.481859\n","test_1    0.880274  0.590141\n","test_2    0.421668  0.300097\n","test_3    0.451560  0.371296\n","test_4    0.341571  0.433553\n","...            ...       ...\n","test_452  0.183908  0.262319\n","test_453  0.191882  0.271128\n","test_454  0.338312  0.313922\n","test_455  0.253139  0.251328\n","test_456  0.410899  0.396222\n","\n","[457 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-83c71a14-0124-494e-8a48-c6212a227a0c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Reorg_g</th>\n","      <th>Reorg_ex</th>\n","    </tr>\n","    <tr>\n","      <th>index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>test_0</th>\n","      <td>0.350062</td>\n","      <td>0.481859</td>\n","    </tr>\n","    <tr>\n","      <th>test_1</th>\n","      <td>0.880274</td>\n","      <td>0.590141</td>\n","    </tr>\n","    <tr>\n","      <th>test_2</th>\n","      <td>0.421668</td>\n","      <td>0.300097</td>\n","    </tr>\n","    <tr>\n","      <th>test_3</th>\n","      <td>0.451560</td>\n","      <td>0.371296</td>\n","    </tr>\n","    <tr>\n","      <th>test_4</th>\n","      <td>0.341571</td>\n","      <td>0.433553</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>test_452</th>\n","      <td>0.183908</td>\n","      <td>0.262319</td>\n","    </tr>\n","    <tr>\n","      <th>test_453</th>\n","      <td>0.191882</td>\n","      <td>0.271128</td>\n","    </tr>\n","    <tr>\n","      <th>test_454</th>\n","      <td>0.338312</td>\n","      <td>0.313922</td>\n","    </tr>\n","    <tr>\n","      <th>test_455</th>\n","      <td>0.253139</td>\n","      <td>0.251328</td>\n","    </tr>\n","    <tr>\n","      <th>test_456</th>\n","      <td>0.410899</td>\n","      <td>0.396222</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>457 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83c71a14-0124-494e-8a48-c6212a227a0c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-83c71a14-0124-494e-8a48-c6212a227a0c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-83c71a14-0124-494e-8a48-c6212a227a0c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["submission.to_csv(path + 'stack.csv')"],"metadata":{"id":"al3Xx8lsld93","executionInfo":{"status":"ok","timestamp":1663068604760,"user_tz":-540,"elapsed":11,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","import lightgbm as lgb\n","\n","LR = MultiOutputRegressor(lgb.LGBMRegressor(random_state=42, n_estimators=1000, max_depth=61, learning_rate=0.075)).fit(train_x, train_y)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1663070568833,"user_tz":-540,"elapsed":46420,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"id":"MzmFTa4V4_or"},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["pred0 = LR.predict(test_x)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1663070570733,"user_tz":-540,"elapsed":12,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"id":"nK-LnYEQ4_ot"},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["pred0.max()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dxnxMjyjE2Xj","executionInfo":{"status":"ok","timestamp":1663070573166,"user_tz":-540,"elapsed":5,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"a4cc1f3d-e280-45df-88e7-29974020e049"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9017997109847191"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["submission[['Reorg_g','Reorg_ex']] = pred"],"metadata":{"id":"SnV7bkzYEipF","executionInfo":{"status":"ok","timestamp":1663070619149,"user_tz":-540,"elapsed":635,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["submission"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"-XqA4mraEpeY","executionInfo":{"status":"ok","timestamp":1663070619670,"user_tz":-540,"elapsed":9,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"2f0f9a58-1958-4632-feaa-130ab9aa007c"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           Reorg_g  Reorg_ex\n","index                       \n","test_0    0.398319  0.399333\n","test_1    0.776101  0.517195\n","test_2    0.378620  0.279672\n","test_3    0.454643  0.341835\n","test_4    0.345880  0.401776\n","...            ...       ...\n","test_452  0.211775  0.258755\n","test_453  0.215280  0.276256\n","test_454  0.283017  0.289056\n","test_455  0.266125  0.258059\n","test_456  0.347004  0.340058\n","\n","[457 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-54523f07-0d2e-462d-bdae-15195dc94e39\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Reorg_g</th>\n","      <th>Reorg_ex</th>\n","    </tr>\n","    <tr>\n","      <th>index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>test_0</th>\n","      <td>0.398319</td>\n","      <td>0.399333</td>\n","    </tr>\n","    <tr>\n","      <th>test_1</th>\n","      <td>0.776101</td>\n","      <td>0.517195</td>\n","    </tr>\n","    <tr>\n","      <th>test_2</th>\n","      <td>0.378620</td>\n","      <td>0.279672</td>\n","    </tr>\n","    <tr>\n","      <th>test_3</th>\n","      <td>0.454643</td>\n","      <td>0.341835</td>\n","    </tr>\n","    <tr>\n","      <th>test_4</th>\n","      <td>0.345880</td>\n","      <td>0.401776</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>test_452</th>\n","      <td>0.211775</td>\n","      <td>0.258755</td>\n","    </tr>\n","    <tr>\n","      <th>test_453</th>\n","      <td>0.215280</td>\n","      <td>0.276256</td>\n","    </tr>\n","    <tr>\n","      <th>test_454</th>\n","      <td>0.283017</td>\n","      <td>0.289056</td>\n","    </tr>\n","    <tr>\n","      <th>test_455</th>\n","      <td>0.266125</td>\n","      <td>0.258059</td>\n","    </tr>\n","    <tr>\n","      <th>test_456</th>\n","      <td>0.347004</td>\n","      <td>0.340058</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>457 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54523f07-0d2e-462d-bdae-15195dc94e39')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-54523f07-0d2e-462d-bdae-15195dc94e39 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-54523f07-0d2e-462d-bdae-15195dc94e39');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["submission.to_csv('/content/drive/MyDrive/Colab Notebooks/Dacon/samsung_ai/data/pred_esm2.csv')"],"metadata":{"id":"Zd6QnOMwz8C9","executionInfo":{"status":"ok","timestamp":1663070619671,"user_tz":-540,"elapsed":7,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["pred1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dacon/samsung_ai/data/pred_10242.csv', index_col = 0)[['Reorg_g', 'Reorg_ex']]\n","pred2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dacon/samsung_ai/data/pred_10243.csv', index_col = 0)[['Reorg_g', 'Reorg_ex']]\n","pred = (pred0 + pred1 + pred2)/3"],"metadata":{"id":"BIEUeKYot9PP","executionInfo":{"status":"ok","timestamp":1663070578744,"user_tz":-540,"elapsed":954,"user":{"displayName":"채윤병","userId":"05417324159184162419"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["class ChemBERTDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, label_df):\n","        self.data = data\n","        self.label = label_df\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx].clone().detach() for key, val in self.data.items()}\n","        item['label'] = torch.tensor([self.label[idx]], dtype = torch.float)\n","        \n","        return item\n","\n","    def __len__(self):\n","        return len(self.data.input_ids)"],"metadata":{"id":"3e2fDyoLyCRq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = 'DeepChem/ChemBERTa-77M-MLM'\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","config = AutoConfig.from_pretrained(model_name)\n","config.num_labels = 1\n","model =  AutoModelForSequenceClassification.from_config(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7pLovHOSxDsI","executionInfo":{"status":"ok","timestamp":1661932264949,"user_tz":-540,"elapsed":8603,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"f2e39e04-6f4f-483e-8239-9c47a086dca1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/DeepChem/ChemBERTa-77M-MLM/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86913776bc30b1494bfe9d7691873f74d55090fa479847495e81111d9105ca63.4e4a24ae990a84c67b78efe89e60d972a98f7c22ca6e9e86f81b5a5d1da1d1ea\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"DeepChem/ChemBERTa-77M-MLM\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.109,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.144,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 464,\n","  \"is_gpu\": true,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 515,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 3,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 600\n","}\n","\n","loading file https://huggingface.co/DeepChem/ChemBERTa-77M-MLM/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/8680e150a2fb64b9d4f4a0eecb870107a8692fac051b1f09a339ca500511fee1.5468158f181199f93da5588d09de73fba27535f90fd445a5a176e3fc5f04919f\n","loading file https://huggingface.co/DeepChem/ChemBERTa-77M-MLM/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/3f597030e9fc8a5e57a239d7f91841c05cc0848c3f7e05af493f5762a2d1270a.90e27582e69797144e6cc9a63c829cad1addb24967933d413b42b370f1abf12a\n","loading file https://huggingface.co/DeepChem/ChemBERTa-77M-MLM/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/04fc8ef68f788604030d9e73d50d8a9a6758f2f7649fbfddad5faf472acbdb57.bcf9c4ad82082f086180ae9806383a789257bf6ccbb246ae63003c741db58ba6\n","loading file https://huggingface.co/DeepChem/ChemBERTa-77M-MLM/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/1f2bf12058d88fc7c118fbb2f0ff810d1927b5f221586e6370785a665b1a0539.c955c51d28400e68b7aaede5ce5dca96c67912c030fb145bc1f142003a4bf11b\n","loading file https://huggingface.co/DeepChem/ChemBERTa-77M-MLM/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/0b572b7c1fa65972fa3090fbb2fd7ae22cc945210e725611a48aa2bbb696ee6d.106100a43b7ce7464c0b8915979ee7c7f36043b18c0119ed38b6f7675f24149c\n","loading file https://huggingface.co/DeepChem/ChemBERTa-77M-MLM/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/51bc6097830c402bb9f140b703fae68cc74220a3f66ef526a02f71a6414138a0.390c54396032ab2c548ce80539ef985160a3a555d14825c9df9f08d7fde34e1f\n","loading configuration file https://huggingface.co/DeepChem/ChemBERTa-77M-MLM/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86913776bc30b1494bfe9d7691873f74d55090fa479847495e81111d9105ca63.4e4a24ae990a84c67b78efe89e60d972a98f7c22ca6e9e86f81b5a5d1da1d1ea\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"DeepChem/ChemBERTa-77M-MLM\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.109,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.144,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 464,\n","  \"is_gpu\": true,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 515,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 3,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 600\n","}\n","\n","loading configuration file https://huggingface.co/DeepChem/ChemBERTa-77M-MLM/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86913776bc30b1494bfe9d7691873f74d55090fa479847495e81111d9105ca63.4e4a24ae990a84c67b78efe89e60d972a98f7c22ca6e9e86f81b5a5d1da1d1ea\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"DeepChem/ChemBERTa-77M-MLM\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.109,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.144,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 464,\n","  \"is_gpu\": true,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 515,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 3,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 600\n","}\n","\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error\n","def compute_metrics(pred):\n","  \"\"\" validation을 위한 metrics function \"\"\"\n","  preds, labels = pred\n","\n","  mse = mean_squared_error(labels, preds)\n","\n","  return {\n","      'mse': mse,\n","  }"],"metadata":{"id":"xeiKp0UM0TJB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split, KFold\n","\n","cv = KFold(n_splits = 5, random_state = 42, shuffle = True)\n","cv_pred2 = []\n","for idx, (train_idx, val_idx) in enumerate(cv.split(train)):\n","  model =  AutoModelForSequenceClassification.from_config(config)\n","  train_set, eval_set = train.iloc[train_idx],  train.iloc[val_idx]\n","\n","  tokenized_train = tokenizer(\n","      list(train_set.SMILES),\n","      return_tensors=\"pt\",\n","      #max_length=128, \n","      padding=True,\n","      truncation=True,\n","      )\n","\n","  tokenized_eval = tokenizer(\n","      list(eval_set.SMILES),\n","      return_tensors=\"pt\",\n","      #max_length=128,\n","      padding=True,\n","      truncation=True,\n","      )\n","\n","  tokenized_test = tokenizer(\n","      list(test.SMILES),\n","      return_tensors=\"pt\",\n","      #max_length=128,\n","      padding=True,\n","      truncation=True,\n","      )\n","\n","  train_dataset = ChemBERTDataset(tokenized_train, train_set['Reorg_ex'])\n","  eval_dataset = ChemBERTDataset(tokenized_eval, eval_set['Reorg_ex'])\n","\n","  training_ars = TrainingArguments(\n","    output_dir='./result',\n","    num_train_epochs=10,\n","    per_device_train_batch_size=64, # 32\n","    learning_rate=5e-4,\n","    weight_decay=0.1,\n","    save_total_limit=5,\n","    evaluation_strategy='epoch',\n","    save_strategy='epoch',\n","    logging_strategy='epoch',\n","    load_best_model_at_end = True,\n","  )\n","\n","  trainer = Trainer(\n","    model=model,\n","    args=training_ars,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n","  )\n","\n","  trainer.train()\n","\n","  num_test = len(tokenized_test['input_ids'])\n","  test_label = torch.zeros(num_test)\n","  test_dataset = ChemBERTDataset(tokenized_test, test_label)\n","\n","  dataloader = DataLoader(test_dataset, batch_size= num_test, shuffle=False)\n","\n","  model.eval()\n","  output_pred = []\n","\n","  for i, data in enumerate(tqdm(dataloader)):\n","      with torch.no_grad():\n","          outputs = model(\n","              input_ids=data['input_ids'].to(device),\n","              attention_mask=data['attention_mask'].to(device),\n","          )\n","      logits = outputs[0]\n","      logits = logits.detach().cpu().numpy()\n","\n","      output_pred.append(logits)\n","  cv_pred2.append(output_pred[0])     \n","\n"],"metadata":{"id":"3RG38969xZFS","executionInfo":{"status":"ok","timestamp":1661936526270,"user_tz":-540,"elapsed":1643144,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2c9daaed-de2a-473d-a027-3bb3e139afdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 14525\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2270\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2270/2270 05:37, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mse</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.133600</td>\n","      <td>0.097822</td>\n","      <td>0.097822</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.104000</td>\n","      <td>0.095602</td>\n","      <td>0.095602</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.098700</td>\n","      <td>0.095938</td>\n","      <td>0.095938</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.094700</td>\n","      <td>0.100200</td>\n","      <td>0.100200</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.092700</td>\n","      <td>0.107869</td>\n","      <td>0.107869</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.090700</td>\n","      <td>0.094788</td>\n","      <td>0.094788</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.087300</td>\n","      <td>0.092221</td>\n","      <td>0.092221</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.084800</td>\n","      <td>0.093808</td>\n","      <td>0.093808</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.079100</td>\n","      <td>0.096088</td>\n","      <td>0.096088</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.075100</td>\n","      <td>0.096932</td>\n","      <td>0.096932</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-227\n","Configuration saved in ./result/checkpoint-227/config.json\n","Model weights saved in ./result/checkpoint-227/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-227/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-227/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-2043] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-454\n","Configuration saved in ./result/checkpoint-454/config.json\n","Model weights saved in ./result/checkpoint-454/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-454/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-454/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-2724] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-681\n","Configuration saved in ./result/checkpoint-681/config.json\n","Model weights saved in ./result/checkpoint-681/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-681/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-681/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-2951] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-908\n","Configuration saved in ./result/checkpoint-908/config.json\n","Model weights saved in ./result/checkpoint-908/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-908/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-908/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-3178] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1135\n","Configuration saved in ./result/checkpoint-1135/config.json\n","Model weights saved in ./result/checkpoint-1135/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1135/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1135/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-3405] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1362\n","Configuration saved in ./result/checkpoint-1362/config.json\n","Model weights saved in ./result/checkpoint-1362/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1362/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1362/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-227] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1589\n","Configuration saved in ./result/checkpoint-1589/config.json\n","Model weights saved in ./result/checkpoint-1589/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1589/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1589/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-454] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1816\n","Configuration saved in ./result/checkpoint-1816/config.json\n","Model weights saved in ./result/checkpoint-1816/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1816/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1816/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-681] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-2043\n","Configuration saved in ./result/checkpoint-2043/config.json\n","Model weights saved in ./result/checkpoint-2043/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-2043/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-2043/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-908] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-2270\n","Configuration saved in ./result/checkpoint-2270/config.json\n","Model weights saved in ./result/checkpoint-2270/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-2270/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-2270/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1135] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./result/checkpoint-1589 (score: 0.09222114831209183).\n","100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 14525\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2270\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2270/2270 05:36, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mse</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.132100</td>\n","      <td>0.098118</td>\n","      <td>0.098118</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.103800</td>\n","      <td>0.098954</td>\n","      <td>0.098954</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.099600</td>\n","      <td>0.098765</td>\n","      <td>0.098765</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.096000</td>\n","      <td>0.097221</td>\n","      <td>0.097221</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.092800</td>\n","      <td>0.095478</td>\n","      <td>0.095478</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.090000</td>\n","      <td>0.094606</td>\n","      <td>0.094606</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.087900</td>\n","      <td>0.100022</td>\n","      <td>0.100022</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.084600</td>\n","      <td>0.095586</td>\n","      <td>0.095586</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.081100</td>\n","      <td>0.097437</td>\n","      <td>0.097437</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.076800</td>\n","      <td>0.099160</td>\n","      <td>0.099160</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-227\n","Configuration saved in ./result/checkpoint-227/config.json\n","Model weights saved in ./result/checkpoint-227/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-227/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-227/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1362] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-454\n","Configuration saved in ./result/checkpoint-454/config.json\n","Model weights saved in ./result/checkpoint-454/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-454/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-454/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1589] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-681\n","Configuration saved in ./result/checkpoint-681/config.json\n","Model weights saved in ./result/checkpoint-681/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-681/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-681/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1816] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-908\n","Configuration saved in ./result/checkpoint-908/config.json\n","Model weights saved in ./result/checkpoint-908/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-908/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-908/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-2043] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1135\n","Configuration saved in ./result/checkpoint-1135/config.json\n","Model weights saved in ./result/checkpoint-1135/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1135/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1135/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-2270] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1362\n","Configuration saved in ./result/checkpoint-1362/config.json\n","Model weights saved in ./result/checkpoint-1362/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1362/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1362/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-227] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1589\n","Configuration saved in ./result/checkpoint-1589/config.json\n","Model weights saved in ./result/checkpoint-1589/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1589/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1589/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-454] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1816\n","Configuration saved in ./result/checkpoint-1816/config.json\n","Model weights saved in ./result/checkpoint-1816/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1816/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1816/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-681] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-2043\n","Configuration saved in ./result/checkpoint-2043/config.json\n","Model weights saved in ./result/checkpoint-2043/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-2043/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-2043/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-908] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3632\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-2270\n","Configuration saved in ./result/checkpoint-2270/config.json\n","Model weights saved in ./result/checkpoint-2270/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-2270/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-2270/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1135] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./result/checkpoint-1362 (score: 0.09460552036762238).\n","100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 14526\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2270\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2270/2270 04:49, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mse</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.136800</td>\n","      <td>0.126962</td>\n","      <td>0.126962</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.105300</td>\n","      <td>0.095525</td>\n","      <td>0.095525</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.099700</td>\n","      <td>0.101493</td>\n","      <td>0.101493</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.096800</td>\n","      <td>0.091754</td>\n","      <td>0.091754</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.093700</td>\n","      <td>0.094342</td>\n","      <td>0.094342</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.091000</td>\n","      <td>0.092218</td>\n","      <td>0.092218</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.087400</td>\n","      <td>0.090275</td>\n","      <td>0.090275</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.083700</td>\n","      <td>0.092374</td>\n","      <td>0.092374</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.080400</td>\n","      <td>0.092111</td>\n","      <td>0.092111</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.075000</td>\n","      <td>0.094251</td>\n","      <td>0.094251</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-227\n","Configuration saved in ./result/checkpoint-227/config.json\n","Model weights saved in ./result/checkpoint-227/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-227/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-227/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1362] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-454\n","Configuration saved in ./result/checkpoint-454/config.json\n","Model weights saved in ./result/checkpoint-454/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-454/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-454/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1589] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-681\n","Configuration saved in ./result/checkpoint-681/config.json\n","Model weights saved in ./result/checkpoint-681/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-681/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-681/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1816] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-908\n","Configuration saved in ./result/checkpoint-908/config.json\n","Model weights saved in ./result/checkpoint-908/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-908/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-908/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-2043] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1135\n","Configuration saved in ./result/checkpoint-1135/config.json\n","Model weights saved in ./result/checkpoint-1135/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1135/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1135/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-2270] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1362\n","Configuration saved in ./result/checkpoint-1362/config.json\n","Model weights saved in ./result/checkpoint-1362/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1362/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1362/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-227] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1589\n","Configuration saved in ./result/checkpoint-1589/config.json\n","Model weights saved in ./result/checkpoint-1589/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1589/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1589/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-454] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1816\n","Configuration saved in ./result/checkpoint-1816/config.json\n","Model weights saved in ./result/checkpoint-1816/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1816/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1816/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-681] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-2043\n","Configuration saved in ./result/checkpoint-2043/config.json\n","Model weights saved in ./result/checkpoint-2043/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-2043/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-2043/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-908] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-2270\n","Configuration saved in ./result/checkpoint-2270/config.json\n","Model weights saved in ./result/checkpoint-2270/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-2270/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-2270/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1135] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./result/checkpoint-1589 (score: 0.09027491509914398).\n","100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 14526\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2270\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2270/2270 05:36, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mse</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.133900</td>\n","      <td>0.100265</td>\n","      <td>0.100265</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.103300</td>\n","      <td>0.098987</td>\n","      <td>0.098987</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.099800</td>\n","      <td>0.100915</td>\n","      <td>0.100915</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.094900</td>\n","      <td>0.098499</td>\n","      <td>0.098499</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.093500</td>\n","      <td>0.095992</td>\n","      <td>0.095992</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.090800</td>\n","      <td>0.095940</td>\n","      <td>0.095940</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.087600</td>\n","      <td>0.095472</td>\n","      <td>0.095472</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.084900</td>\n","      <td>0.098248</td>\n","      <td>0.098248</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.081100</td>\n","      <td>0.097737</td>\n","      <td>0.097737</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.076600</td>\n","      <td>0.097385</td>\n","      <td>0.097385</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-227\n","Configuration saved in ./result/checkpoint-227/config.json\n","Model weights saved in ./result/checkpoint-227/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-227/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-227/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1362] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-454\n","Configuration saved in ./result/checkpoint-454/config.json\n","Model weights saved in ./result/checkpoint-454/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-454/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-454/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1589] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-681\n","Configuration saved in ./result/checkpoint-681/config.json\n","Model weights saved in ./result/checkpoint-681/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-681/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-681/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1816] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-908\n","Configuration saved in ./result/checkpoint-908/config.json\n","Model weights saved in ./result/checkpoint-908/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-908/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-908/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-2043] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1135\n","Configuration saved in ./result/checkpoint-1135/config.json\n","Model weights saved in ./result/checkpoint-1135/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1135/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1135/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-2270] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1362\n","Configuration saved in ./result/checkpoint-1362/config.json\n","Model weights saved in ./result/checkpoint-1362/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1362/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1362/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-227] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1589\n","Configuration saved in ./result/checkpoint-1589/config.json\n","Model weights saved in ./result/checkpoint-1589/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1589/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1589/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-454] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1816\n","Configuration saved in ./result/checkpoint-1816/config.json\n","Model weights saved in ./result/checkpoint-1816/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1816/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1816/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-681] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-2043\n","Configuration saved in ./result/checkpoint-2043/config.json\n","Model weights saved in ./result/checkpoint-2043/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-2043/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-2043/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-908] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-2270\n","Configuration saved in ./result/checkpoint-2270/config.json\n","Model weights saved in ./result/checkpoint-2270/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-2270/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-2270/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1135] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./result/checkpoint-1589 (score: 0.09547242522239685).\n","100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 14526\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2270\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2270/2270 05:32, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mse</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.136200</td>\n","      <td>0.110974</td>\n","      <td>0.110974</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.104800</td>\n","      <td>0.098886</td>\n","      <td>0.098886</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.099900</td>\n","      <td>0.094113</td>\n","      <td>0.094113</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.096700</td>\n","      <td>0.094591</td>\n","      <td>0.094591</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.094000</td>\n","      <td>0.093843</td>\n","      <td>0.093843</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.090700</td>\n","      <td>0.096051</td>\n","      <td>0.096051</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.088700</td>\n","      <td>0.096482</td>\n","      <td>0.096482</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.084900</td>\n","      <td>0.093311</td>\n","      <td>0.093311</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.080800</td>\n","      <td>0.092934</td>\n","      <td>0.092934</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.077200</td>\n","      <td>0.094983</td>\n","      <td>0.094983</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-227\n","Configuration saved in ./result/checkpoint-227/config.json\n","Model weights saved in ./result/checkpoint-227/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-227/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-227/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1362] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-454\n","Configuration saved in ./result/checkpoint-454/config.json\n","Model weights saved in ./result/checkpoint-454/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-454/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-454/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1589] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-681\n","Configuration saved in ./result/checkpoint-681/config.json\n","Model weights saved in ./result/checkpoint-681/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-681/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-681/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1816] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-908\n","Configuration saved in ./result/checkpoint-908/config.json\n","Model weights saved in ./result/checkpoint-908/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-908/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-908/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-2043] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1135\n","Configuration saved in ./result/checkpoint-1135/config.json\n","Model weights saved in ./result/checkpoint-1135/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1135/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1135/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-2270] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1362\n","Configuration saved in ./result/checkpoint-1362/config.json\n","Model weights saved in ./result/checkpoint-1362/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1362/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1362/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-227] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1589\n","Configuration saved in ./result/checkpoint-1589/config.json\n","Model weights saved in ./result/checkpoint-1589/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1589/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1589/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-454] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1816\n","Configuration saved in ./result/checkpoint-1816/config.json\n","Model weights saved in ./result/checkpoint-1816/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1816/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1816/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-681] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-2043\n","Configuration saved in ./result/checkpoint-2043/config.json\n","Model weights saved in ./result/checkpoint-2043/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-2043/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-2043/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-908] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3631\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-2270\n","Configuration saved in ./result/checkpoint-2270/config.json\n","Model weights saved in ./result/checkpoint-2270/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-2270/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-2270/special_tokens_map.json\n","Deleting older checkpoint [result/checkpoint-1135] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./result/checkpoint-2043 (score: 0.09293358772993088).\n","100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n"]}]},{"cell_type":"code","source":["submission.Reorg_ex = np.mean(cv_pred2, axis = 0)"],"metadata":{"id":"No6PlXHQzxV7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission"],"metadata":{"id":"DLn-YPquvZhC","executionInfo":{"status":"ok","timestamp":1661936528394,"user_tz":-540,"elapsed":11,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"colab":{"base_uri":"https://localhost:8080/","height":455},"outputId":"a9d92193-33ec-44a0-f6f9-070dba06a2be"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           Reorg_g  Reorg_ex\n","index                       \n","test_0    0.404351  0.391127\n","test_1    0.699554  0.545267\n","test_2    0.564711  0.476591\n","test_3    0.762356  0.762603\n","test_4    0.324132  0.284140\n","...            ...       ...\n","test_452  0.274878  0.224002\n","test_453  0.283318  0.220352\n","test_454  0.283416  0.228271\n","test_455  0.321170  0.246026\n","test_456  0.264595  0.203103\n","\n","[457 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-481601bb-8e72-4ee9-a8ae-46d0148a09bd\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Reorg_g</th>\n","      <th>Reorg_ex</th>\n","    </tr>\n","    <tr>\n","      <th>index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>test_0</th>\n","      <td>0.404351</td>\n","      <td>0.391127</td>\n","    </tr>\n","    <tr>\n","      <th>test_1</th>\n","      <td>0.699554</td>\n","      <td>0.545267</td>\n","    </tr>\n","    <tr>\n","      <th>test_2</th>\n","      <td>0.564711</td>\n","      <td>0.476591</td>\n","    </tr>\n","    <tr>\n","      <th>test_3</th>\n","      <td>0.762356</td>\n","      <td>0.762603</td>\n","    </tr>\n","    <tr>\n","      <th>test_4</th>\n","      <td>0.324132</td>\n","      <td>0.284140</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>test_452</th>\n","      <td>0.274878</td>\n","      <td>0.224002</td>\n","    </tr>\n","    <tr>\n","      <th>test_453</th>\n","      <td>0.283318</td>\n","      <td>0.220352</td>\n","    </tr>\n","    <tr>\n","      <th>test_454</th>\n","      <td>0.283416</td>\n","      <td>0.228271</td>\n","    </tr>\n","    <tr>\n","      <th>test_455</th>\n","      <td>0.321170</td>\n","      <td>0.246026</td>\n","    </tr>\n","    <tr>\n","      <th>test_456</th>\n","      <td>0.264595</td>\n","      <td>0.203103</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>457 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-481601bb-8e72-4ee9-a8ae-46d0148a09bd')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-481601bb-8e72-4ee9-a8ae-46d0148a09bd button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-481601bb-8e72-4ee9-a8ae-46d0148a09bd');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":406}]},{"cell_type":"code","source":["submission.to_csv(path + 'pred1.csv')"],"metadata":{"id":"-Z0yOIfwFcT-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = ChemBERTDataset(tokenized_train, train_dataset['Reorg_g'])\n","eval_dataset = ChemBERTDataset(tokenized_eval, eval_dataset['Reorg_g'])"],"metadata":{"id":"rL-Wa-kTznRR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = next(iter(eval_dataset))\n","data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ajFiXk-BMqm","executionInfo":{"status":"ok","timestamp":1661928154112,"user_tz":-540,"elapsed":13,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"2d8126f1-e777-47b6-91ff-2f5ec92eb9cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([12, 16, 19, 15, 20, 15, 15, 17, 39, 16, 22, 16, 17, 39, 34, 18, 16, 17,\n","         22, 19, 18, 19, 18, 15, 15, 17, 54, 18, 15, 20, 19, 16, 16, 17, 23, 18,\n","         22, 19, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n"," 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n"," 'label': tensor([1.2405])}"]},"metadata":{},"execution_count":355}]},{"cell_type":"code","source":["train_loader = DataLoader(train_dataset, batch_size = 64)\n","eval_loader = DataLoader(eval_dataset, batch_size = 32)"],"metadata":{"id":"4xyVVT1iw1RB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data= next(iter(train_loader))\n","data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b6GtejioZrMB","executionInfo":{"status":"ok","timestamp":1661928154114,"user_tz":-540,"elapsed":13,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"fd500dd4-4ed0-40b9-a1e0-477b3de2fdbc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[12, 19, 22,  ...,  0,  0,  0],\n","         [12, 16, 16,  ...,  0,  0,  0],\n","         [12, 16, 23,  ...,  0,  0,  0],\n","         ...,\n","         [12, 16, 16,  ...,  0,  0,  0],\n","         [12, 16, 39,  ...,  0,  0,  0],\n","         [12, 23, 16,  ...,  0,  0,  0]]),\n"," 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         ...,\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0]]),\n"," 'label': tensor([[0.9420],\n","         [0.0754],\n","         [1.0972],\n","         [0.7301],\n","         [1.0212],\n","         [0.9012],\n","         [0.2736],\n","         [0.3856],\n","         [0.7078],\n","         [0.1532],\n","         [0.2484],\n","         [0.1809],\n","         [0.3691],\n","         [0.3106],\n","         [0.9631],\n","         [0.1434],\n","         [0.3478],\n","         [0.5353],\n","         [1.4660],\n","         [0.7554],\n","         [0.5365],\n","         [1.0584],\n","         [0.7673],\n","         [0.1409],\n","         [0.5407],\n","         [0.5534],\n","         [0.7698],\n","         [0.8125],\n","         [1.0984],\n","         [0.5556],\n","         [0.9373],\n","         [0.9494],\n","         [0.7558],\n","         [0.4508],\n","         [0.9740],\n","         [0.4121],\n","         [1.0492],\n","         [1.3048],\n","         [0.5589],\n","         [0.6216],\n","         [0.5463],\n","         [0.7299],\n","         [0.6783],\n","         [0.2303],\n","         [0.4180],\n","         [1.3880],\n","         [0.8229],\n","         [0.7040],\n","         [0.3911],\n","         [1.2696],\n","         [1.2359],\n","         [0.3947],\n","         [1.4245],\n","         [1.1358],\n","         [0.1111],\n","         [1.1942],\n","         [0.7099],\n","         [0.5651],\n","         [0.1759],\n","         [0.5051],\n","         [0.7132],\n","         [0.9874],\n","         [0.5778],\n","         [1.1656]])}"]},"metadata":{},"execution_count":357}]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(params =  model.parameters(), lr= 1e-4)\n","\n","def loss_fn(outputs, targets):\n","    return torch.nn.MSELoss()(outputs, targets)\n","\n","def trainer(epoch):\n","    model.to(device)\n","    model.train()\n","    min_loss = np.inf\n","    for e in range(epoch):\n","      for _, (data) in enumerate(train_loader, 0):\n","        ids = data['input_ids'].to(device, dtype = torch.long)\n","        mask = data['attention_mask'].to(device, dtype = torch.long)\n","\n","        targets = data['label'].to(device, dtype = torch.float)\n","\n","        outputs = model(ids, mask)\n","\n","        optimizer.zero_grad()\n","        loss = loss_fn(outputs.logits, targets)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","      print(f'Epoch: {e+1}, Loss:  {loss.item()}')\n","      if loss < min_loss:\n","        min_loss = loss\n","        val_loss = validation()\n","        print(f'Epoch: {e+1}, Val_Loss:  {val_loss.item()}')\n","      \n","\n","def validation():\n","    model.eval()\n","\n","    fin_outputs=[]\n","    with torch.no_grad():\n","        for _, (data) in enumerate(eval_loader, 0):\n","            ids = data['input_ids'].to(device, dtype = torch.long)\n","            mask = data['attention_mask'].to(device, dtype = torch.long)\n","\n","            targets = data['label'].to(device, dtype = torch.float)\n","            outputs = model(ids, mask)\n","            loss = loss_fn(outputs.logits, targets)\n","\n","    return loss\n"],"metadata":{"id":"4EZBmlyJNR6C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":549},"id":"G5eWvw79eU8s","executionInfo":{"status":"error","timestamp":1661931296485,"user_tz":-540,"elapsed":889795,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"20f5fd69-58e1-455a-aa73-b7813aa1eae0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Loss:  0.13384997844696045\n","Epoch: 1, Val_Loss:  0.12674137949943542\n","Epoch: 2, Loss:  0.12413898855447769\n","Epoch: 2, Val_Loss:  0.11923918128013611\n","Epoch: 3, Loss:  0.11315642297267914\n","Epoch: 3, Val_Loss:  0.09465809911489487\n","Epoch: 4, Loss:  0.10693023353815079\n","Epoch: 4, Val_Loss:  0.09483005851507187\n","Epoch: 5, Loss:  0.10187987983226776\n","Epoch: 5, Val_Loss:  0.096208855509758\n","Epoch: 6, Loss:  0.09389141947031021\n","Epoch: 6, Val_Loss:  0.10159632563591003\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-383-831128280fe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-382-28e34ddca76f>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {e+1}, Loss:  {loss.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["num_test = len(tokenized_test['input_ids'])\n","test_label_df = pd.DataFrame(np.zeros(num_test), columns = ['Reorg_g'])\n","test_label_df['Reorg_ex'] = 0.0"],"metadata":{"id":"Goj8WdbVgBbC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_label_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"cMaXSaXlg3lL","executionInfo":{"status":"ok","timestamp":1661927084931,"user_tz":-540,"elapsed":6,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"aa3fc7b2-6d95-4c1d-b872-7633f88e9398"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Reorg_g  Reorg_ex\n","0        0.0       0.0\n","1        0.0       0.0\n","2        0.0       0.0\n","3        0.0       0.0\n","4        0.0       0.0\n","..       ...       ...\n","452      0.0       0.0\n","453      0.0       0.0\n","454      0.0       0.0\n","455      0.0       0.0\n","456      0.0       0.0\n","\n","[457 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-bbf26644-ea49-4748-b578-c6fd19cc92c7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Reorg_g</th>\n","      <th>Reorg_ex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>452</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>453</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>454</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>455</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>456</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>457 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbf26644-ea49-4748-b578-c6fd19cc92c7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bbf26644-ea49-4748-b578-c6fd19cc92c7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bbf26644-ea49-4748-b578-c6fd19cc92c7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":292}]},{"cell_type":"code","source":["test_dataset = ChemBERTDataset(tokenized_test, test_label_df)\n","test_loader = DataLoader(test_dataset)"],"metadata":{"id":"_5qzSMvggBbD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test():\n","    model.eval()\n","\n","    fin_outputs=[]\n","    with torch.no_grad():\n","        for _, (data,label) in enumerate(test_loader, 0):\n","            ids = data['input_ids'].to(device, dtype = torch.long)\n","            mask = data['attention_mask'].to(device, dtype = torch.long)\n","\n","            outputs = model(ids, mask)\n","            fin_outputs.extend(outputs.logits.cpu().detach().numpy().tolist())\n","    return fin_outputs"],"metadata":{"id":"kHGEs3A1fkoh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred = test()"],"metadata":{"id":"2G-ufvXjgBCU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_ars = TrainingArguments(\n","    output_dir='./result',\n","    num_train_epochs=7,\n","    per_device_train_batch_size=32, # 64\n","    learning_rate=5e-5,\n","    weight_decay=0.1,\n","    save_total_limit=5,\n","    save_steps=500,\n","    evaluation_strategy='steps',\n","    eval_steps = 500,\n","    load_best_model_at_end = True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_ars,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":428},"id":"ZxETvbyx0zdN","executionInfo":{"status":"ok","timestamp":1661920600041,"user_tz":-540,"elapsed":1058,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"e5e97c7f-5110-48af-f976-50127a1e436f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 2\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7/7 00:00, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=7, training_loss=0.6849952425275531, metrics={'train_runtime': 0.3898, 'train_samples_per_second': 35.914, 'train_steps_per_second': 17.957, 'total_flos': 66772565160.0, 'train_loss': 0.6849952425275531, 'epoch': 7.0})"]},"metadata":{},"execution_count":132}]},{"cell_type":"code","source":[],"metadata":{"id":"dkY1Ehzh1zD7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TDaK9etu1Xut"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_test = len(tokenized_test['input_ids'])\n","test_label = torch.zeros(num_test)\n","test_dataset = ChemBERTDataset(tokenized_test, test_label)\n","\n","dataloader = DataLoader(test_dataset, batch_size= num_test, shuffle=False)\n","\n","model.eval()\n","output_pred = []\n","\n","for i, data in enumerate(tqdm(dataloader)):\n","    with torch.no_grad():\n","        outputs = model(\n","            input_ids=data['input_ids'].to(device),\n","            attention_mask=data['attention_mask'].to(device),\n","        )\n","    logits = outputs[0]\n","    logits = logits.detach().cpu().numpy()\n","\n","    output_pred.append(logits) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rr4Ugogs0P0Z","executionInfo":{"status":"ok","timestamp":1661848033198,"user_tz":-540,"elapsed":622,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"7c29a7e2-fd99-475d-9b3a-7796cfb8aa0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n","100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n"]}]},{"cell_type":"code","source":["submission.Reorg_ex = output_pred[0]"],"metadata":{"id":"sgVC-urL2sS2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission.to_csv(path + 'pred.csv')"],"metadata":{"id":"2q95sFNmvZjh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","ATOM_LIST = ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'unk']\n","def onek_encoding(x, allowable_set):\n","    if x not in allowable_set:                                                                                                                                               \n","        raise Exception('input {0} not in allowable set{1}:'.format(x, allowable_set))\n","    return list(map(lambda s: x == s, allowable_set))\n","\n","def onek_encoding_unk(x, allowable_set):\n","    if x not in allowable_set:\n","        x = allowable_set[-1]\n","    return list(map(lambda s: x == s, allowable_set))\n","\n","def atom_features(atom):\n","        return np.array(onek_encoding_unk(atom.GetSymbol(), ATOM_LIST)\n","                        + onek_encoding_unk(atom.GetDegree(), [0, 1, 2, 3, 4, 5])\n","                        + onek_encoding_unk(atom.GetExplicitValence(), [1, 2, 3, 4, 5, 6])\n","                        + onek_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5])\n","                        + [atom.GetIsAromatic()], dtype=np.float32)"],"metadata":{"id":"Z3FzUI5iKffe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["atom = []\n","for a in mol.GetAtoms():\n","  atom.append(atom_features(a).reshape(1, -1))\n","  #except: atom.append(np.zeros(82).reshape(1, -1))\n","atom = np.vstack(atom)"],"metadata":{"id":"t_fBZYFOJvo-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["atom[-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X9yDDrsXOPsW","executionInfo":{"status":"ok","timestamp":1660814329657,"user_tz":-540,"elapsed":10,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"8910275e-453b-4544-cfc5-f57bdf035f73"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n","       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["a.GetIsAromatic()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpAbd2wUK0D8","executionInfo":{"status":"ok","timestamp":1660813954899,"user_tz":-540,"elapsed":7,"user":{"displayName":"채윤병","userId":"05417324159184162419"}},"outputId":"929679c4-9550-4ab7-eb8c-fdd27b2e12b6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":[],"metadata":{"id":"wwQWpUgYNsmm"},"execution_count":null,"outputs":[]}]}